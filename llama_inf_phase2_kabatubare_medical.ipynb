{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA Supervised Fine-Tuning\n",
    "\n",
    "This document will take the answers of GPT-4o on the Kababutare Medical Dataset and then fine-tune the LLaMA Model on those answers.\n",
    "\n",
    "The purpose of this exercise is to test whether the LLaMA fine-tuning is able to distill the knowledge of GPT-4o and improve the performance on the open-ended question/answering related to healthcare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from tqdm  import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Question and Answer Pairs from Test Dataset Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = []\n",
    "ans_list = []\n",
    "llama_resp_list = []\n",
    "gpt_resp_list = []\n",
    "\n",
    "with open('phase2_data_kabatubare/test_kabatubare.jsonl', 'rb') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        ques_list.append(json_object['question'])\n",
    "        ans_list.append(json_object['answer'])\n",
    "        llama_resp_list.append(json_object['llama_response_base'])\n",
    "        gpt_resp_list.append(json_object['gpt_response_base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({'question': ques_list,\n",
    "                          'answer': ans_list,\n",
    "                          'llama_response_base': llama_resp_list,\n",
    "                          'gpt_response_base': gpt_resp_list})\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_path = \"./llama32-sft-full-kabatubare\"\n",
    "peft_model_path = \"./llama32-sft-peft-kabatubare\" #use for LoRA based fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = peft_model_path,\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing sample-by-sample inference. (Batch Inference doesn't work well for fine-tuned model adapters as responses like `P P P P` are being produced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response_ft(question_input: str):\n",
    "    \n",
    "    llama_input = [{\"role\": \"system\", \"content\": \"You are a medical knowledge assistant trained to provide information and guidance on various health-related topics.\"},\n",
    "                    {\"role\": \"user\", \"content\": question_input}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(llama_input, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "    temp_resp = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=2048,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    resp = resp[len(temp_resp):] #getting only the response part (i.e., assistant)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Unsloth Fast Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "llama_responses_ft = []\n",
    "for index, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "    question_input = row['question']\n",
    "    llama_resp = get_llama_response_ft(question_input)\n",
    "    llama_responses_ft.append(llama_resp)\n",
    "\n",
    "with open('phase2_kabatubare_medical/llama_responses_ft.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_responses_ft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('phase2_kabatubare_medical/llama_responses_ft.pkl', 'rb') as file:\n",
    "    llama_responses_ft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the LLaMA Fine-Tuned Responses into the complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['llama_responses_ft'] = llama_responses_ft\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
