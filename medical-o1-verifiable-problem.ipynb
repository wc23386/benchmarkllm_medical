{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will evaluate LLM on medical dataset and use Opik to track the process.\n",
    "We use `llama3.2:3b` and the dataset can be found here: https://huggingface.co/datasets/FreedomIntelligence/medical-o1-verifiable-problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to get started on Opik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We'll start by creating an account on [comet.com](https://www.comet.com/site/?ref=dailydoseofds.com).\n",
    "\n",
    "2. Once you create an account, it will give you two options to choose fromâ€”select LLM evaluation (Opik).\n",
    "\n",
    "3. Once done, you will find yourself in your dashboard, where you can also find your API key on the right.\n",
    "\n",
    "4. Next, in your current working directory, create a `.env` file. Copy the API key shown in your dashboard and paste it as follows: `COMET_API_KEY=\"your-api-key-here\"`.\n",
    "\n",
    "5. To configure Opik, run the following code in a new Python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opik\n",
    "# opik.configure(use_local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above code will open a panel to enter the API key obtained above. Enter the API key there, and Opik has been configured. Or you can set the API key in the Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up Opik environment variables\n",
    "# os.environ[\"OPIK_API_KEY\"] = \"YOUR_API_KEY\"  # Replace with your API key\n",
    "# os.environ[\"OPIK_WORKSPACE\"] = \"YOUR_WROKSPACE_HERE\"  # Replace with your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A step-by-step guide on using Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to [Ollama.com](https://ollama.com/?ref=dailydoseofds.com), select your operating system. I'm using macOS, so I directly download it.\n",
    "\n",
    "2. After installing, run the command: \"ollama serve\". \n",
    "\n",
    "3. Choose the model you're looking for and in another terminal, run: \"ollama run [YOUR_MODEL_HERE]\". This will download the model locally. I'm using `llama 3.2:3b`.\n",
    "\n",
    "4. Finally, install the open-source Opik framework, LlamaIndex, and LlamaIndex's Ollama integration module as follows: \n",
    "```bash\n",
    "pip install opik\n",
    "pip install llama-index\n",
    "pip install llama-index-llms-ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wcchang/Documents/testenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contstruct to get the response.\n",
    "\n",
    "The decorator `@track` is all you need to track the LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik import track\n",
    "\n",
    "@track(project_name=\"medical_dataset\")\n",
    "def get_llama_response(question, max_retries=3):\n",
    "    \"\"\"Get response from locally running Llama model via Ollama\"\"\"\n",
    "    prompt = f\"You are a medical knowledge assistant trained to provide information and guidance on various health-related topics.\\nGive the direct answer without any explanation. Question: {question}\\nAnswer:\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post('http://localhost:11434/api/generate',\n",
    "                                   json={\n",
    "                                       'model': 'llama3.2:3b',\n",
    "                                       'prompt': prompt,\n",
    "                                       'stream': False,\n",
    "                                       'options': {\n",
    "                                           'mps': True  # I'm using Apple Silicon GPU\n",
    "                                       }\n",
    "                                   },\n",
    "                                   timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response'].strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise Exception(f\"Failed to get response after {max_retries} attempts: {str(e)}\")\n",
    "            time.sleep(2 * (attempt + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the metrics using BLUE and ROUGE\n",
    "\n",
    "`evaluate` is Hugging Face's official library for:\n",
    "\n",
    "1. Loading and computing NLP metrics with consistent APIs\n",
    "\n",
    "2. Using standard implementations (same as in papers and benchmarks)\n",
    "\n",
    "3. Supporting BLEU, ROUGE, METEOR, BERTScore, Exact Match, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "\n",
    "@track(project_name=\"medical_dataset\")\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"Calculate BLEU and ROUGE scores for a batch\"\"\"\n",
    "    bleu_eval = evaluate.load(\"bleu\")\n",
    "    bleu_results = bleu_eval.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    rouge_eval = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge_eval.compute(predictions=predictions, references=[r[0] for r in references])\n",
    "    \n",
    "    return {**bleu_results, **rouge_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track(project_name=\"medical_dataset\")\n",
    "def main():\n",
    "    # Load medical dataset\n",
    "    try:\n",
    "        medical_dataset = load_dataset(\"FreedomIntelligence/medical-o1-verifiable-problem\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    total_entries = len(medical_dataset['train'])\n",
    "    batch_size = 200\n",
    "    \n",
    "    # Initialize lists for storing responses\n",
    "    llama_responses = []\n",
    "    answer_list = []\n",
    "    error_count = 0\n",
    "    \n",
    "    print(f\"\\nProcessing {total_entries} entries in batches of {batch_size}:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Process all examples with progress bar\n",
    "    for i, example in enumerate(tqdm(medical_dataset['train'], total=total_entries)):\n",
    "        if error_count >= 10:\n",
    "            print(\"\\nToo many errors encountered. Stopping processing.\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            question = example['Open-ended Verifiable Question']\n",
    "            ground_truth = example['Ground-True Answer']\n",
    "            \n",
    "            prediction = get_llama_response(question=question)\n",
    "            llama_responses.append(prediction)\n",
    "            answer_list.append([ground_truth])\n",
    "            \n",
    "            # Print first 5 Q&As\n",
    "            if i < 5:\n",
    "                print(f\"\\nQ{i+1}: {question}\")\n",
    "                print(f\"A{i+1}: {prediction}\")\n",
    "                print(f\"Ground Truth: {ground_truth}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"\\nError processing entry {i}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate metrics every batch_size entries\n",
    "        if (i + 1) % batch_size == 0 and llama_responses:\n",
    "            metrics = calculate_metrics(llama_responses, answer_list)\n",
    "            \n",
    "            print(f\"\\nMetrics after {i + 1} entries:\")\n",
    "            print(\"BLEU Score:\", metrics['bleu'])\n",
    "            print(\"ROUGE Scores:\")\n",
    "            print(f\"  ROUGE-1: {metrics['rouge1']:.4f}\")\n",
    "            print(f\"  ROUGE-2: {metrics['rouge2']:.4f}\")\n",
    "            print(f\"  ROUGE-L: {metrics['rougeL']:.4f}\")\n",
    "            print(f\"Errors encountered: {error_count}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    if llama_responses:\n",
    "        print(\"\\nFinal Metrics:\")\n",
    "        final_metrics = calculate_metrics(llama_responses, answer_list)\n",
    "        final_metrics['total_errors'] = error_count\n",
    "        print(json.dumps(final_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "    # main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
